{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버전 기록\n",
    "- Model Baseline 코드 (2024.12.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples : NUM_CELL_LINES(1280) * NUM_DRUGS (193)\n",
    "num_pathways = 245\n",
    "num_genes = 231\n",
    "num_drugs = 69\n",
    "num_substructures = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('data.pt')\n",
    "\n",
    "gene_embeddings = data['gene_embeddings']  # Shape: [1280, 245, 231]\n",
    "gene_adjacencies = data['gene_adjacencies']  # Shape: [245, 231, 231]\n",
    "substructure_embeddings = data['substructure_embeddings']  # Shape: [69, 245, 193] (drug_num x num_pathways x num_substructures)\n",
    "substructure_adjacencies = data['substructure_adjacencies']  # Shape: [69, 245, 193, 193]\n",
    "labels = torch.randint(0, 2, (1280, 69), dtype=torch.float32)  # Shape: [1280, 69] (cell_line_num x drug_num)\n",
    "\n",
    "\n",
    "# 모든 cell line과 drug의 조합 생성\n",
    "cell_line_indices = range(len(gene_embeddings))  # 1280\n",
    "drug_indices = range(num_drugs)  # 69\n",
    "sample_indices = list(product(cell_line_indices, drug_indices))  # [(0, 0), (0, 1), ..., (1279, 68)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugResponseDataset(Dataset):\n",
    "    def __init__(self, gene_embeddings, gene_adjacencies, substructure_embeddings, substructure_adjacencies, labels, sample_indices):\n",
    "        self.gene_embeddings = gene_embeddings  # [1280, 245, 231]\n",
    "        self.gene_adjacencies = gene_adjacencies  # [245, 231, 231]\n",
    "        self.substructure_embeddings = substructure_embeddings  # [245, 193]\n",
    "        self.substructure_adjacencies = substructure_adjacencies  # [245, 193, 193]\n",
    "        self.labels = labels  # [1280, 69]\n",
    "        self.sample_indices = sample_indices  # [(cell_line_idx, drug_idx), ...]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cell_line_idx, drug_idx = self.sample_indices[idx]\n",
    "        return {\n",
    "            'gene_embedding': self.gene_embeddings[cell_line_idx],  # [245, 231]\n",
    "            'gene_adj': self.gene_adjacencies,                     # [245, 231, 231] (공유)\n",
    "            'substructure_embedding': self.substructure_embeddings,  # [245, 193] (공유)\n",
    "            'substructure_adj': self.substructure_adjacencies[drug_idx],  # [245, 193, 193]\n",
    "            'label': self.labels[cell_line_idx, drug_idx]           # Scalar\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    gene_embeddings = []\n",
    "    gene_adjacencies = []\n",
    "    substructure_embeddings = []\n",
    "    substructure_adjacencies = []\n",
    "    labels = []\n",
    "\n",
    "    for item in batch:\n",
    "        gene_embeddings.append(item['gene_embedding'])\n",
    "        gene_adjacencies.append(item['gene_adj'])\n",
    "        substructure_embeddings.append(item['substructure_embedding'])\n",
    "        substructure_adjacencies.append(item['substructure_adj'])\n",
    "        labels.append(item['label'])\n",
    "\n",
    "    return {\n",
    "        'gene_embeddings': torch.stack(gene_embeddings),  # [batch_size, num_pathways, num_genes]\n",
    "        'gene_adjacencies': torch.stack(gene_adjacencies),  # [batch_size, num_pathways, num_genes, num_genes]\n",
    "        'substructure_embeddings': torch.stack(substructure_embeddings),  # [batch_size, num_pathways, num_substructures]\n",
    "        'substructure_adjacencies': torch.stack(substructure_adjacencies),  # [batch_size, num_pathways, num_substructures, num_substructures]\n",
    "        'labels': torch.tensor(labels, dtype=torch.float32)  # [batch_size]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 초기화\n",
    "dataset = DrugResponseDataset(\n",
    "    gene_embeddings=gene_embeddings,\n",
    "    gene_adjacencies=gene_adjacencies,\n",
    "    substructure_embeddings=substructure_embeddings,\n",
    "    substructure_adjacencies=substructure_adjacencies,\n",
    "    labels=labels,\n",
    "    sample_indices=sample_indices \n",
    ")\n",
    "\n",
    "# DataLoader 초기화\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 데이터 확인\n",
    "for batch in data_loader:\n",
    "    print(f\"Batch gene embeddings: {batch['gene_embeddings'].shape}\")  # [8, 245, 231]\n",
    "    print(f\"Batch gene adjacencies: {batch['gene_adjacencies'].shape}\")  # [8, 245, 231, 231]\n",
    "    print(f\"Batch substructure embeddings: {batch['substructure_embeddings'].shape}\")  # [8, 245, 193]\n",
    "    print(f\"Batch substructure adjacencies: {batch['substructure_adjacencies'].shape}\")  # [8, 245, 193, 193]\n",
    "    print(f\"Batch labels: {batch['labels'].shape}\")  # [8]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) Embedding Layer\n",
    "### - GeneEmbeddingLayer : FloatTensor -> Linear\n",
    "### - SubstructureEmbeddingLayer : IntTensor -> nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENE_EMBEDDING_DIM = 128\n",
    "SUBSTRUCTURE_EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "FINAL_DIM = 64\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneEmbeddingLayer(nn.Module):\n",
    "    # In)  [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_GENES(231)] \n",
    "    # Out) [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_GENES(231), GENE_EMBEDDING_DIM(128)]\n",
    "\n",
    "    def __init__(self, num_genes, embedding_dim=GENE_EMBEDDING_DIM):\n",
    "        super(GeneEmbeddingLayer, self).__init__()\n",
    "        self.linear = nn.Linear(1, embedding_dim)  \n",
    "        self.num_genes = num_genes\n",
    "        \n",
    "    def forward(self, gene_values):\n",
    "        gene_values = gene_values.view(-1, 1) # [BATCH_SIZE * NUM_PATHWAYS * NUM_GENES, 1]\n",
    "        embedded_values = self.linear(gene_values)  # [BATCH_SIZE * NUM_PATHWAYS * NUM_GENES, GENE_EMBEDDING_DIM]\n",
    "        return embedded_values.view(-1, num_pathways, num_genes, GENE_EMBEDDING_DIM) \n",
    "\n",
    "class SubstructureEmbeddingLayer(nn.Module):\n",
    "    # In)  [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_SUBSTRUCTURES(193)]\n",
    "    # Out) [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_SUBSTRUCTURES(193), SUBSTRUCTURES_EMBEDDING_DIM(128)]\n",
    "    \n",
    "    def __init__(self, num_substructures, embedding_dim=SUBSTRUCTURE_EMBEDDING_DIM):\n",
    "        super(SubstructureEmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_substructures, embedding_dim) # [NUM_SUBSTRUCTURES, SUBSTRUCTURES_EMBEDDING_DIM]\n",
    "\n",
    "    def forward(self, substructure_indices):\n",
    "        return self.embedding(substructure_indices) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(query_dim, query_dim)  \n",
    "        self.key_layer = nn.Linear(key_dim, query_dim)      \n",
    "        self.value_layer = nn.Linear(key_dim, query_dim)    \n",
    "\n",
    "    def forward(self, query_embeddings, key_embeddings):\n",
    "        query = self.query_layer(query_embeddings) \n",
    "        key = self.key_layer(key_embeddings)        \n",
    "        value = self.value_layer(key_embeddings)   \n",
    "\n",
    "        # Attention Scores\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))  \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)        \n",
    "\n",
    "        # Apply Attention\n",
    "        attended_embeddings = torch.matmul(attention_weights, value)  \n",
    "        \n",
    "        return attended_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## (2) Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GraphEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GraphEmbedding, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, embeddings, adj_matrices):\n",
    "        # adjacency matrices -> edge_index and batch tensors\n",
    "        batch_graphs = []\n",
    "        batch_size = embeddings.size(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            adj_matrix = adj_matrices[i]\n",
    "            edge_index = adj_matrix.nonzero(as_tuple=False).t()  # [2, num_edges]\n",
    "            node_features = embeddings[i]  # [num_nodes, input_dim]\n",
    "            \n",
    "            # Create graph data for each batch\n",
    "            graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "            batch_graphs.append(graph_data)\n",
    "        \n",
    "        # Batch all graphs together\n",
    "        batch = Batch.from_data_list(batch_graphs)\n",
    "        \n",
    "        # Apply GCN layers\n",
    "        x = self.conv1(batch.x, batch.edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, batch.edge_index)\n",
    "        \n",
    "        # Global mean pooling for graph-level embedding\n",
    "        graph_embedding = global_mean_pool(x, batch.batch)  # [batch_size, hidden_dim]\n",
    "        return graph_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) DrugResponseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugResponseModel(nn.Module):\n",
    "    def __init__(self, num_genes, num_substructures, hidden_dim, final_dim):\n",
    "        super(DrugResponseModel, self).__init__()\n",
    "        # Embedding Layers\n",
    "        self.gene_embedding_layer = GeneEmbeddingLayer(num_genes, GENE_EMBEDDING_DIM)\n",
    "        self.substructure_embedding_layer = SubstructureEmbeddingLayer(num_substructures, SUBSTRUCTURE_EMBEDDING_DIM)\n",
    "        \n",
    "        # Cross Attention Layers\n",
    "        self.Gene2Sub_cross_attention = CrossAttention(query_dim=GENE_EMBEDDING_DIM, key_dim=SUBSTRUCTURE_EMBEDDING_DIM)\n",
    "        self.Sub2Gene_cross_attention = CrossAttention(query_dim=SUBSTRUCTURE_EMBEDDING_DIM, key_dim=GENE_EMBEDDING_DIM)\n",
    "\n",
    "        # Graph Embedding Layers\n",
    "        self.pathway_graph = GraphEmbedding(GENE_EMBEDDING_DIM, hidden_dim)\n",
    "        self.drug_graph = GraphEmbedding(SUBSTRUCTURE_EMBEDDING_DIM, hidden_dim)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(2 * hidden_dim, final_dim)\n",
    "        self.fc2 = nn.Linear(final_dim, OUTPUT_DIM)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, gene_embeddings, gene_adjacencies, substructure_embeddings, substructure_adjacencies):\n",
    "        # Dummy code for making integer tensor\n",
    "        substructure_embeddings = substructure_embeddings.int() \n",
    "\n",
    "        # Gene and Substructure Embeddings\n",
    "        gene_embeddings = self.gene_embedding_layer(gene_embeddings)  # [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_GENES(231), GENE_EMBEDDING_DIM(128)]\n",
    "        substructure_embeddings = self.substructure_embedding_layer(substructure_embeddings)  # [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_SUBSTRUCTURES(193), SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "        # Pathway and Drug Graph Embeddings List\n",
    "        pathway_graph_embeddings = []\n",
    "        drug_graph_embeddings = []\n",
    "\n",
    "        # Pathway loop\n",
    "        for i in range(gene_embeddings.size(1)):  \n",
    "            gene_emb = gene_embeddings[:, i, :]  # [BATCH_SIZE(8), NUM_GENES(231), GENE_EMBEDDING_DIM(128)]\n",
    "            sub_emb = substructure_embeddings[:, i, :]  # [BATCH_SIZE(8), NUM_SUBSTRUCTURES(193), SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "            # Cross attention\n",
    "            updated_gene_emb = self.Gene2Sub_cross_attention(gene_emb, sub_emb) # [BATCH_SIZE(8), NUM_GENES(231), GENE_EMBEDDING_DIM(128)]\n",
    "            updated_sub_emb = self.Sub2Gene_cross_attention(sub_emb, gene_emb) # [BATCH_SIZE(8), NUM_SUBSTRUCTURES(193), SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "            # Generate graph embeddings for each pathway\n",
    "            pathway_graph_embedding = self.pathway_graph(updated_gene_emb, gene_adjacencies[:, i, :, :]) # [BATCH_SIZE(8),GENE_EMBEDDING_DIM(128)]\n",
    "            # drug_graph_embedding = self.drug_graph(updated_sub_emb, substructure_adjacencies[:, i, :, :]) # [BATCH_SIZE(8),SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "            drug_graph_embedding = self.drug_graph(updated_sub_emb, substructure_adjacencies) # [BATCH_SIZE(8),SUBSTRUCTURE_EMBEDDING_DIM(128)] - adj 모든 배치에 대해 동일\n",
    "\n",
    "            pathway_graph_embeddings.append(pathway_graph_embedding)\n",
    "            drug_graph_embeddings.append(drug_graph_embedding)\n",
    "\n",
    "        # Stack pathway and drug graph embeddings\n",
    "        pathway_graph_embeddings = torch.stack(pathway_graph_embeddings, dim=1) # [BATCH_SIZE(8), NUM_PATHWAYS(10), GENE_EMBEDDING_DIM(128)]\n",
    "        drug_graph_embeddings = torch.stack(drug_graph_embeddings, dim=1) # [BATCH_SIZE(8), NUM_PATHWAYS(10), SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "        # Mean pooling\n",
    "        final_pathway_embedding = torch.mean(pathway_graph_embeddings, dim=1) # [BATCH_SIZE(8), GENE_EMBEDDING_DIM(128)]\n",
    "        final_drug_embedding = torch.mean(drug_graph_embeddings, dim=1) # [BATCH_SIZE(8), SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "        # Concatenate final embeddings\n",
    "        combined_embedding = torch.cat((final_pathway_embedding, final_drug_embedding), dim=-1)  # [BATCH_SIZE(8), GENE_EMBEDDING_DIM + SUBSTRUCTURE_EMBEDDING_DIM(256)]\n",
    "\n",
    "        # Final Prediction\n",
    "        x = self.fc1(combined_embedding) # [BATCH_SIZE(8), FINAL_DIM(64)]\n",
    "        x = self.fc2(x) # [BATCH_SIZE(8), OUTPUT_DIM(1)]\n",
    "\n",
    "        return self.sigmoid(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DrugResponseModel(num_genes, num_substructures, HIDDEN_DIM, FINAL_DIM)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        gene_embeddings = batch['gene_embeddings'].to(device)\n",
    "        gene_adjacencies = batch['gene_adjacencies'].to(device)\n",
    "        substructure_embeddings = batch['substructure_embeddings'].to(device)\n",
    "        substructure_adjacencies = batch['substructure_adjacencies'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(gene_embeddings, gene_adjacencies, substructure_embeddings, substructure_adjacencies)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(data_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drTrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
