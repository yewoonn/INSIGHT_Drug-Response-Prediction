{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버전 기록\n",
    "- Model Baseline 코드 (2024.12.30)\n",
    "- Drug Graph, Drug Graph Embedding Block 추가 (2025.01.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "import random\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # num_samples : NUM_CELL_LINES(1280) * NUM_DRUGS (193)\n",
    "num_pathways = 245\n",
    "num_genes = 231\n",
    "num_drugs = 83\n",
    "num_substructures = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cell_lines = 10  # Reduced number for simplicity\n",
    "# num_pathways = 5\n",
    "# num_genes = 15\n",
    "# num_drugs = 7  # Reduced number for simplicity\n",
    "# num_substructures = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Gene Embedding Dictionary \n",
    "Cell Line → gene embeddings (num_pathways, num_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_embeddings = np.load('../0. input/gene_embeddings.npz')\n",
    "gene_embeddings = {\n",
    "    key: torch.tensor(saved_embeddings[key], dtype=torch.float32)\n",
    "    for key in saved_embeddings.keys()\n",
    "}\n",
    "saved_embeddings.close()\n",
    "\n",
    "# print(gene_embeddings['DATA.683665'].shape)\n",
    "len(gene_embeddings)\n",
    "\n",
    "# gene_embeddings = {\n",
    "#     f\"DATA.{i}\": torch.rand(num_pathways, num_genes, dtype=torch.float32)\n",
    "#     for i in range(num_cell_lines)\n",
    "# }\n",
    "\n",
    "# gene_embeddings['DATA.1'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Drug Embedding Dictionary\n",
    "CID (str) → drug embeddings (num_substructures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_embeddings = np.load('../0. input/0_drug_embeddings.npz')\n",
    "drug_embeddings = {\n",
    "    key: torch.tensor(saved_embeddings[key], dtype=torch.float32)\n",
    "    for key in saved_embeddings.keys()\n",
    "}\n",
    "saved_embeddings.close()\n",
    "\n",
    "len(drug_embeddings)\n",
    "\n",
    "# drug_embeddings = {\n",
    "#     f\"{i}\": torch.rand(num_substructures, dtype=torch.float32)\n",
    "#     for i in range(num_drugs)\n",
    "# }\n",
    "\n",
    "# drug_embeddings['1'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Drug Graph Dictionary\n",
    "CID → drug graph (pytorch geomeric data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77648/80507385.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  drug_graph_dict = torch.load('../0. input/0_drug_graph_dict.pt')\n"
     ]
    }
   ],
   "source": [
    "drug_graph_dict = torch.load('../0. input/0_drug_graph_dict.pt')\n",
    "\n",
    "print(len(drug_graph_dict))\n",
    "\n",
    "# # Create dummy data for drug_graphs\n",
    "# drug_graph_dict = {\n",
    "#     f\"{i}\": Data(\n",
    "#         x=torch.rand(num_substructures, 5),  # Node features\n",
    "#         edge_index=torch.randint(0, num_substructures, (2, 12)),  # Random edges\n",
    "#         global_ids=list(range(num_substructures))\n",
    "#     )\n",
    "#     for i in range(num_drugs)\n",
    "# }\n",
    "\n",
    "# print(drug_graph_dict['1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77648/461541504.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('data.pt')\n"
     ]
    }
   ],
   "source": [
    "data = torch.load('data.pt')\n",
    "# gene_embeddings = data['gene_embeddings']  # Shape: [1280, 245, 231]\n",
    "gene_adjacencies = data['gene_adjacencies']  # Shape: [245, 231, 231]\n",
    "# substructure_embeddings = data['substructure_embeddings']  # Shape: [69, 245, 193] (drug_num x num_pathways x num_substructures)\n",
    "\n",
    "# gene_adjacencies = torch.rand(num_pathways, num_genes, num_genes)\n",
    "# substructure_embeddings = {\n",
    "#     drug: torch.rand(10) for drug in drug_embeddings.keys()\n",
    "# }\n",
    "\n",
    "# 모든 cell line과 drug의 조합 생성\n",
    "cell_lines = list(gene_embeddings.keys())\n",
    "drugs = list(drug_embeddings.keys())\n",
    "sample_indices = list(product(cell_lines, drugs)) \n",
    "\n",
    "# Cell line과 drug의 매핑\n",
    "cell_line_mapping = {key: idx for idx, key in enumerate(gene_embeddings.keys())}\n",
    "drug_mapping = {cid: idx for idx, cid in enumerate(drug_graph_dict.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77648/2449569987.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels_dict = torch.load('../0. input/0_drug_label_dict.pt')\n"
     ]
    }
   ],
   "source": [
    "labels_dict = torch.load('../0. input/0_drug_label_dict.pt')\n",
    "\n",
    "print(len(labels_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugResponseDataset(Dataset):\n",
    "    def __init__(self, gene_embeddings, gene_adjacencies, substructure_embeddings, drug_graphs, labels, sample_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gene_embeddings (dict): {cell_line_id: Tensor}, Gene embeddings for each cell line.\n",
    "            drug_graphs (dict): List of PyTorch Geometric Data objects for each drug (indexed by drug_id).\n",
    "            substructure_embeddings (Tensor): [245, 193], Substructure embeddings for pathways.\n",
    "            labels (dict): {cell_line_id: Tensor}, Drug response labels for each cell line and drug pair.\n",
    "            sample_indices (list): [(cell_line_id, drug_idx)], List of cell line and drug index pairs.\n",
    "        \"\"\"\n",
    "        self.gene_embeddings = gene_embeddings  # {cell_line_id: [245, 231]}\n",
    "        self.gene_adjacencies = gene_adjacencies\n",
    "        self.drug_graphs = drug_graphs  # Drug graphs\n",
    "        self.substructure_embeddings = substructure_embeddings  # [170]\n",
    "        self.labels = labels  # {cell_line_id, drug_id : [1]}\n",
    "        self.sample_indices = sample_indices  # [(cell_line_id, drug_id)]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cell_line_id, drug_id = self.sample_indices[idx]\n",
    "\n",
    "        # Gene embeddings for the cell line\n",
    "        gene_embedding = self.gene_embeddings[cell_line_id]  # [245, 231]\n",
    "\n",
    "        # Substructure embeddings for pathways\n",
    "        substructure_embedding = self.substructure_embeddings[drug_id].repeat(245, 1)  # [245, 170]\n",
    "        drug_graph = self.drug_graphs[drug_id]  # Drug graphs\n",
    "\n",
    "        # Get the label for the cell line-drug pair\n",
    "        label = self.labels[cell_line_id, drug_id]  # Scalar\n",
    "\n",
    "        return {\n",
    "            'gene_embedding': gene_embedding,  # [245, 231]\n",
    "            'gene_adj': self.gene_adjacencies,                     # 더미\n",
    "            'substructure_embedding': substructure_embedding,  # [245, 170]\n",
    "            'drug_graph': drug_graph,  # PyTorch Geometric Data object\n",
    "            'label': label  # Scalar\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    gene_embeddings = []\n",
    "    gene_adjacencies = []\n",
    "    substructure_embeddings = []\n",
    "    drug_graphs = []\n",
    "    labels = []\n",
    "\n",
    "    for item in batch:\n",
    "        gene_embeddings.append(item['gene_embedding'])\n",
    "        gene_adjacencies.append(item['gene_adj'])\n",
    "        substructure_embeddings.append(item['substructure_embedding'])\n",
    "        drug_graphs.append(item['drug_graph'])\n",
    "        labels.append(item['label'])\n",
    "\n",
    "    drug_batch = Batch.from_data_list(drug_graphs)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'gene_embeddings': torch.stack(gene_embeddings),  # [batch_size, num_pathways, num_genes]\n",
    "        'gene_adjacencies': torch.stack(gene_adjacencies),  # [batch_size, num_pathways, num_genes, num_genes]\n",
    "        'substructure_embeddings': torch.stack(substructure_embeddings),  # [batch_size, num_pathways, num_substructures]\n",
    "        'drug_graphs': drug_batch,  # [batch_size, num_pathways, num_substructures, num_substructures]\n",
    "        'labels': torch.tensor(labels, dtype=torch.float32)  # [batch_size]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 초기화\n",
    "dataset = DrugResponseDataset(\n",
    "    gene_embeddings=gene_embeddings,\n",
    "    gene_adjacencies=gene_adjacencies,\n",
    "    substructure_embeddings=drug_embeddings,\n",
    "    drug_graphs=drug_graph_dict,\n",
    "    labels=labels_dict,\n",
    "    sample_indices=sample_indices,\n",
    ")\n",
    "\n",
    "# DataLoader 초기화\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch gene embeddings: torch.Size([8, 245, 231])\n",
      "Batch gene adjacencies: torch.Size([8, 245, 231, 231])\n",
      "Batch substructure embeddings: torch.Size([8, 245, 201])\n",
      "Batch drug graphs: DataBatch(x=[56, 128], edge_index=[2, 51], drug=[8], node_names=[8], global_ids=[8], batch=[56], ptr=[9])\n",
      "Batch labels: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "for batch in data_loader:\n",
    "    print(f\"Batch gene embeddings: {batch['gene_embeddings'].shape}\")  # [8, 245, 231]\n",
    "    print(f\"Batch gene adjacencies: {batch['gene_adjacencies'].shape}\")  # [8, 245, 231, 231]\n",
    "    print(f\"Batch substructure embeddings: {batch['substructure_embeddings'].shape}\")  # [8, 245, 193]\n",
    "    print(f\"Batch drug graphs: {batch['drug_graphs']}\")  # [8, 245, 193, 193]\n",
    "    print(f\"Batch labels: {batch['labels'].shape}\")  # [8]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) Embedding Layer\n",
    "### - GeneEmbeddingLayer : FloatTensor -> Linear\n",
    "### - SubstructureEmbeddingLayer : IntTensor -> nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENE_EMBEDDING_DIM = 128\n",
    "SUBSTRUCTURE_EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "FINAL_DIM = 64\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneEmbeddingLayer(nn.Module):\n",
    "    # In)  [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_GENES(231)] \n",
    "    # Out) [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_GENES(231), GENE_EMBEDDING_DIM(128)]\n",
    "\n",
    "    def __init__(self, num_genes, embedding_dim=GENE_EMBEDDING_DIM):\n",
    "        super(GeneEmbeddingLayer, self).__init__()\n",
    "        self.linear = nn.Linear(1, embedding_dim)  \n",
    "        self.num_genes = num_genes\n",
    "        \n",
    "    def forward(self, gene_values):\n",
    "        gene_values = gene_values.view(-1, 1) # [BATCH_SIZE * NUM_PATHWAYS * NUM_GENES, 1]\n",
    "        embedded_values = self.linear(gene_values)  # [BATCH_SIZE * NUM_PATHWAYS * NUM_GENES, GENE_EMBEDDING_DIM]\n",
    "        return embedded_values.view(-1, num_pathways, num_genes, GENE_EMBEDDING_DIM) \n",
    "\n",
    "class SubstructureEmbeddingLayer(nn.Module):\n",
    "    # In)  [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_SUBSTRUCTURES(193)]\n",
    "    # Out) [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_SUBSTRUCTURES(193), SUBSTRUCTURES_EMBEDDING_DIM(128)]\n",
    "    \n",
    "    def __init__(self, num_substructures, embedding_dim=SUBSTRUCTURE_EMBEDDING_DIM):\n",
    "        super(SubstructureEmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_substructures, embedding_dim) # [NUM_SUBSTRUCTURES, SUBSTRUCTURES_EMBEDDING_DIM]\n",
    "\n",
    "    def forward(self, substructure_indices):\n",
    "        return self.embedding(substructure_indices) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(query_dim, query_dim)  \n",
    "        self.key_layer = nn.Linear(key_dim, query_dim)      \n",
    "        self.value_layer = nn.Linear(key_dim, query_dim)    \n",
    "\n",
    "    def forward(self, query_embeddings, key_embeddings):\n",
    "        query = self.query_layer(query_embeddings) \n",
    "        key = self.key_layer(key_embeddings)        \n",
    "        value = self.value_layer(key_embeddings)   \n",
    "\n",
    "        # Attention Scores\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))  \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)        \n",
    "\n",
    "        # Apply Attention\n",
    "        attended_embeddings = torch.matmul(attention_weights, value)  \n",
    "        \n",
    "        return attended_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## (2) Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GraphEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GraphEmbedding, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, embeddings, adj_matrices):\n",
    "        # adjacency matrices -> edge_index and batch tensors\n",
    "        batch_graphs = []\n",
    "        batch_size = embeddings.size(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            adj_matrix = adj_matrices[i]\n",
    "            edge_index = adj_matrix.nonzero(as_tuple=False).t()  # [2, num_edges]\n",
    "            node_features = embeddings[i]  # [num_nodes, input_dim]\n",
    "            \n",
    "            # Create graph data for each batch\n",
    "            graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "            batch_graphs.append(graph_data)\n",
    "        \n",
    "        # Batch all graphs together\n",
    "        batch = Batch.from_data_list(batch_graphs)\n",
    "        \n",
    "        # Apply GCN layers\n",
    "        x = self.conv1(batch.x, batch.edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, batch.edge_index)\n",
    "        \n",
    "        # Global mean pooling for graph-level embedding\n",
    "        graph_embedding = global_mean_pool(x, batch.batch)  # [batch_size, hidden_dim]\n",
    "        return graph_embedding\n",
    "    \n",
    "\n",
    "class DrugGraphEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(DrugGraphEmbedding, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, drug_graphs, drug_graph_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            drug_graphs (Batch): Batched PyTorch Geometric Data object.\n",
    "            drug_graph_embedding (Tensor): [BATCH_SIZE, NUM_PATHWAYS, NUM_SUBSTRUCTURES, EMBEDDING_DIM]\n",
    "        \"\"\"\n",
    "        all_node_features = []\n",
    "        updated_node_features = torch.mean(drug_graph_embedding, dim=1)  # [BATCH_SIZE(8), NUM_SUBSTRUCTURES, SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "        # Batch Loop\n",
    "        for batch_idx in range(updated_node_features.size(0)):  \n",
    "            # Get global IDs and node indices\n",
    "            global_ids = drug_graphs[batch_idx].global_ids  \n",
    "            node_indices = torch.where(drug_graphs.batch == batch_idx)[0]\n",
    "\n",
    "            # Ensure global_ids and node_indices match in length\n",
    "            assert len(global_ids) == len(node_indices), \"Mismatch between global IDs and node indices length\"\n",
    "\n",
    "            # Update node features for the current batch\n",
    "            node_features = torch.zeros((len(node_indices), updated_node_features.size(-1)), device=updated_node_features.device)\n",
    "            for local_idx, global_id in enumerate(global_ids):\n",
    "                if global_id < updated_node_features.size(1):\n",
    "                    node_features[local_idx] = updated_node_features[batch_idx, global_id]\n",
    "\n",
    "            # Append the current batch's node features to the list\n",
    "            all_node_features.append(node_features)\n",
    "\n",
    "        # updated node features from all batches\n",
    "        new_node_features = torch.cat(all_node_features, dim=0)  # Shape: [TOTAL_NUM_NODES, EMBEDDING_DIM]\n",
    "        drug_graphs.x = new_node_features\n",
    "\n",
    "        # GCN layers\n",
    "        x = self.conv1(drug_graphs.x, drug_graphs.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, drug_graphs.edge_index)\n",
    "\n",
    "        # Perform global mean pooling\n",
    "        graph_embedding = global_mean_pool(x, drug_graphs.batch)  # Shape: [BATCH_SIZE, HIDDEN_DIM]\n",
    "        return graph_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) DrugResponseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugResponseModel(nn.Module):\n",
    "    def __init__(self, num_genes, num_substructures, hidden_dim, final_dim):\n",
    "        super(DrugResponseModel, self).__init__()\n",
    "        # Embedding Layers\n",
    "        self.gene_embedding_layer = GeneEmbeddingLayer(num_genes, GENE_EMBEDDING_DIM)\n",
    "        self.substructure_embedding_layer = SubstructureEmbeddingLayer(num_substructures, SUBSTRUCTURE_EMBEDDING_DIM)\n",
    "        \n",
    "        # Cross Attention Layers\n",
    "        self.Gene2Sub_cross_attention = CrossAttention(query_dim=GENE_EMBEDDING_DIM, key_dim=SUBSTRUCTURE_EMBEDDING_DIM)\n",
    "        self.Sub2Gene_cross_attention = CrossAttention(query_dim=SUBSTRUCTURE_EMBEDDING_DIM, key_dim=GENE_EMBEDDING_DIM)\n",
    "\n",
    "        # Graph Embedding Layers\n",
    "        self.pathway_graph = GraphEmbedding(GENE_EMBEDDING_DIM, hidden_dim)\n",
    "        self.drug_graph = DrugGraphEmbedding(SUBSTRUCTURE_EMBEDDING_DIM, hidden_dim)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(2 * hidden_dim, final_dim)\n",
    "        self.fc2 = nn.Linear(final_dim, OUTPUT_DIM)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, gene_embeddings, gene_adjacencies, substructure_embeddings, drug_graphs):\n",
    "        # Dummy code for making integer tensor\n",
    "        substructure_embeddings = substructure_embeddings.int() \n",
    "\n",
    "        # Gene and Substructure Embeddings\n",
    "        gene_embeddings = self.gene_embedding_layer(gene_embeddings)  # [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_GENES(231), GENE_EMBEDDING_DIM(128)]\n",
    "        substructure_embeddings = self.substructure_embedding_layer(substructure_embeddings)  # [BATCH_SIZE(8), NUM_PATHWAYS(10), NUM_SUBSTRUCTURES(193), SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "        # Pathway and Drug Graph Embeddings List\n",
    "        pathway_graph_embeddings = []\n",
    "        drug_graph_embeddings = []\n",
    "\n",
    "        # Pathway loop\n",
    "        for i in range(gene_embeddings.size(1)):  \n",
    "            gene_emb = gene_embeddings[:, i, :]  # [BATCH_SIZE(8), NUM_GENES(231), GENE_EMBEDDING_DIM(128)]\n",
    "            sub_emb = substructure_embeddings[:, i, :]  # [BATCH_SIZE(8), NUM_SUBSTRUCTURES(193), SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "            # # Cross attention\n",
    "            # updated_gene_emb = self.Gene2Sub_cross_attention(gene_emb, sub_emb) # [BATCH_SIZE(8), NUM_GENES(231), GENE_EMBEDDING_DIM(128)]\n",
    "            # updated_sub_emb = self.Sub2Gene_cross_attention(sub_emb, gene_emb) # [BATCH_SIZE(8), NUM_SUBSTRUCTURES(193), SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "\n",
    "            # # Generate graph embeddings for each pathway\n",
    "            # pathway_graph_embedding = self.pathway_graph(updated_gene_emb, gene_adjacencies[:, i, :, :]) # [BATCH_SIZE(8),GENE_EMBEDDING_DIM(128)]\n",
    "            # pathway_graph_embeddings.append(pathway_graph_embedding)\n",
    "\n",
    "            # drug_graph_embeddings.append(updated_sub_emb)\n",
    "\n",
    "            pathway_graph_embedding = self.pathway_graph(gene_emb, gene_adjacencies[:, i, :, :]) # [BATCH_SIZE(8),GENE_EMBEDDING_DIM(128)]\n",
    "            pathway_graph_embeddings.append(pathway_graph_embedding)\n",
    "\n",
    "            drug_graph_embeddings.append(sub_emb)\n",
    "        \n",
    "        \n",
    "        # Drug Graph Embedding for all pathways\n",
    "        drug_graph_embedding = torch.stack(drug_graph_embeddings, dim=1)  # [BATCH_SIZE(8), NUM_PATHWAYS, NUM_SUBSTRUCTURES, SUBSTRUCTURE_EMBEDDING_DIM(128)]\n",
    "        final_drug_embedding = self.drug_graph(drug_graphs, drug_graph_embedding)  # [BATCH_SIZE(8), HIDDEN_DIM(128)]\n",
    "\n",
    "        # Stack pathway and drug graph embeddings\n",
    "        pathway_graph_embeddings = torch.stack(pathway_graph_embeddings, dim=1) # [BATCH_SIZE(8), NUM_PATHWAYS(10), GENE_EMBEDDING_DIM(128)]\n",
    "        final_pathway_embedding = torch.mean(pathway_graph_embeddings, dim=1) # [BATCH_SIZE(8), GENE_EMBEDDING_DIM(128)]\n",
    "\n",
    "        # Concatenate final embeddings\n",
    "        combined_embedding = torch.cat((final_pathway_embedding, final_drug_embedding), dim=-1)  # [BATCH_SIZE(8), GENE_EMBEDDING_DIM + SUBSTRUCTURE_EMBEDDING_DIM(256)]\n",
    "\n",
    "        # Final Prediction\n",
    "        x = self.fc1(combined_embedding) # [BATCH_SIZE(8), FINAL_DIM(64)]\n",
    "        x = self.fc2(x) # [BATCH_SIZE(8), OUTPUT_DIM(1)]\n",
    "\n",
    "        return self.sigmoid(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DrugResponseModel(num_genes, num_substructures, HIDDEN_DIM, FINAL_DIM)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        gene_embeddings = batch['gene_embeddings'].to(device)\n",
    "        gene_adjacencies = batch['gene_adjacencies'].to(device)\n",
    "        substructure_embeddings = batch['substructure_embeddings'].to(device)\n",
    "        drug_graphs = batch['drug_graphs'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(gene_embeddings, gene_adjacencies, substructure_embeddings, drug_graphs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(data_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drTrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
